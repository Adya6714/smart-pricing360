{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e65c72",
   "metadata": {},
   "source": [
    "# Minimal Effective Pipeline for Price Prediction\n",
    "Focus: Only what matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2133f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.utils.seed import seed_everything\n",
    "from src.training.metrics import smape_np\n",
    "from src.data.load import read_csvs, make_folds\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = torch.device(os.getenv(\"DEVICE\", \"mps\"))\n",
    "TRAIN_CSV = str(project_root / os.getenv(\"TRAIN_CSV\", \"dataset/train.csv\"))\n",
    "TEST_CSV = str(project_root / os.getenv(\"TEST_CSV\", \"dataset/test.csv\"))\n",
    "IMG_DIR = project_root / Path(os.getenv(\"IMG_DIR\", \"data/processed/images\"))\n",
    "OUT_DIR = project_root / \"outputs_minimal\"\n",
    "HF_HOME = project_root / \".hf_cache\"\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "HF_HOME.mkdir(exist_ok=True)\n",
    "seed_everything(SEED)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aee24e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: /Users/adyasrivastava/Downloads/smart_pricing/dataset/train.csv\n",
      "Loading test data from: /Users/adyasrivastava/Downloads/smart_pricing/dataset/test.csv\n",
      "\n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "Price statistics:\n",
      "  Min: $0.13\n",
      "  Max: $2796.00\n",
      "  Mean: $23.65\n",
      "  Median: $14.00\n",
      "\n",
      "Images available:\n",
      "  Train: 75000/75000 (100.0%)\n",
      "  Test: 75000/75000 (100.0%)\n",
      "\n",
      "Created 5 stratified folds:\n",
      "  Fold 0: 15000 samples, mean price: $23.46, median: $14.10\n",
      "  Fold 1: 15000 samples, mean price: $23.63, median: $13.99\n",
      "  Fold 2: 15000 samples, mean price: $23.47, median: $14.20\n",
      "  Fold 3: 15000 samples, mean price: $23.96, median: $13.99\n",
      "  Fold 4: 15000 samples, mean price: $23.73, median: $14.05\n",
      "Train: 75000, Test: 75000\n",
      "Holdout: 63750 train, 11250 val\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df, test_df = read_csvs(TRAIN_CSV, TEST_CSV)\n",
    "train_df = make_folds(train_df, n_folds=5, seed=SEED)\n",
    "\n",
    "# Holdout split\n",
    "y_log = np.log1p(train_df[\"price\"].clip(lower=0.0))\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=SEED)\n",
    "tr_idx, va_idx = next(sss.split(np.zeros(len(y_log)), pd.qcut(y_log, q=20, duplicates=\"drop\")))\n",
    "hold_tr = train_df.iloc[tr_idx].copy()\n",
    "hold_va = train_df.iloc[va_idx].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(f\"Holdout: {len(hold_tr)} train, {len(hold_va)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2e0fc",
   "metadata": {},
   "source": [
    "# Critical Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9aa6208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 features\n"
     ]
    }
   ],
   "source": [
    "def extract_smart_features(text):\n",
    "    \"\"\"Extract only the most predictive features\"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Quantities (most important!)\n",
    "    weight_kg = sum([float(m.group(1)) for m in re.finditer(r'(\\d+\\.?\\d*)\\s*kg', text)])\n",
    "    weight_g = sum([float(m.group(1)) for m in re.finditer(r'(\\d+\\.?\\d*)\\s*g\\b', text)])\n",
    "    volume_l = sum([float(m.group(1)) for m in re.finditer(r'(\\d+\\.?\\d*)\\s*l\\b', text)])\n",
    "    volume_ml = sum([float(m.group(1)) for m in re.finditer(r'(\\d+\\.?\\d*)\\s*ml', text)])\n",
    "    \n",
    "    # Convert to standard units\n",
    "    total_g = weight_kg * 1000 + weight_g\n",
    "    total_ml = volume_l * 1000 + volume_ml\n",
    "    \n",
    "    # Pack size\n",
    "    pack = 1\n",
    "    for pattern in [r'(\\d+)\\s*pack', r'(\\d+)\\s*ct\\b']:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            pack = max(pack, max([int(m) for m in matches]))\n",
    "    \n",
    "    # Simple text stats\n",
    "    words = len(text.split())\n",
    "    numbers = len(re.findall(r'\\d+', text))\n",
    "    \n",
    "    # Premium indicator\n",
    "    premium = int(any(w in text for w in ['organic', 'premium', 'natural', 'gourmet']))\n",
    "    \n",
    "    return {\n",
    "        'weight_g': total_g,\n",
    "        'volume_ml': total_ml,\n",
    "        'pack_count': pack,\n",
    "        'log_weight': np.log1p(total_g),\n",
    "        'log_volume': np.log1p(total_ml),\n",
    "        'word_count': words,\n",
    "        'num_count': numbers,\n",
    "        'is_premium': premium,\n",
    "    }\n",
    "\n",
    "train_df = train_df.join(train_df['catalog_content'].fillna('').apply(extract_smart_features).apply(pd.Series))\n",
    "test_df = test_df.join(test_df['catalog_content'].fillna('').apply(extract_smart_features).apply(pd.Series))\n",
    "hold_tr = hold_tr.join(hold_tr['catalog_content'].fillna('').apply(extract_smart_features).apply(pd.Series))\n",
    "hold_va = hold_va.join(hold_va['catalog_content'].fillna('').apply(extract_smart_features).apply(pd.Series))\n",
    "\n",
    "feature_cols = ['weight_g', 'volume_ml', 'pack_count', 'log_weight', \n",
    "                'log_volume', 'word_count', 'num_count', 'is_premium', 'has_image']\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef91c89",
   "metadata": {},
   "source": [
    "# Best Transformer: sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [1:29:53<00:00,  4.60s/it]  \n",
      "Encoding:  10%|â–‰         | 115/1172 [06:16<55:56,  3.18s/it]  "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import trange\n",
    "\n",
    "# This is consistently one of the best general-purpose encoders\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "print(f\"Loading: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=str(HF_HOME))\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, cache_dir=str(HF_HOME)).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def mean_pool(output, mask):\n",
    "    embeddings = output[0]\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    return torch.sum(embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(texts, batch_size=64):\n",
    "    all_emb = []\n",
    "    for i in trange(0, len(texts), batch_size, desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        out = model(**enc)\n",
    "        pooled = mean_pool(out, enc['attention_mask'])\n",
    "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "        all_emb.append(pooled.cpu().numpy())\n",
    "    return np.vstack(all_emb).astype(np.float32)\n",
    "\n",
    "# Encode\n",
    "texts_train = train_df['catalog_content'].fillna('').astype(str).tolist()\n",
    "texts_test = test_df['catalog_content'].fillna('').astype(str).tolist()\n",
    "texts_htr = hold_tr['catalog_content'].fillna('').astype(str).tolist()\n",
    "texts_hva = hold_va['catalog_content'].fillna('').astype(str).tolist()\n",
    "\n",
    "emb_train = encode(texts_train)\n",
    "emb_test = encode(texts_test)\n",
    "emb_htr = encode(texts_htr)\n",
    "emb_hva = encode(texts_hva)\n",
    "\n",
    "print(f\"Embedding shape: {emb_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ac982",
   "metadata": {},
   "source": [
    "# LightGBM Only (Fast + Effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\", \"-q\"])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "print(\"\\nTraining LightGBM with 5-fold CV...\")\n",
    "\n",
    "# Combine embeddings + features (no scaling for trees!)\n",
    "X_train = np.hstack([emb_train, train_df[feature_cols].values]).astype(np.float32)\n",
    "X_test = np.hstack([emb_test, test_df[feature_cols].values]).astype(np.float32)\n",
    "\n",
    "oof = np.zeros(len(train_df))\n",
    "test_pred = np.zeros(len(test_df))\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'verbose': -1,\n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"\\nFold {fold+1}/5:\")\n",
    "    tr_idx = train_df[train_df.fold != fold].index\n",
    "    va_idx = train_df[train_df.fold == fold].index\n",
    "    \n",
    "    # Train in log space\n",
    "    train_set = lgb.Dataset(X_train[tr_idx], np.log1p(train_df.loc[tr_idx, 'price']))\n",
    "    val_set = lgb.Dataset(X_train[va_idx], np.log1p(train_df.loc[va_idx, 'price']))\n",
    "    \n",
    "    gbm = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_set],\n",
    "        callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict in price space\n",
    "    oof[va_idx] = np.expm1(gbm.predict(X_train[va_idx]))\n",
    "    test_pred += np.expm1(gbm.predict(X_test))\n",
    "    \n",
    "    fold_smape = smape_np(train_df.loc[va_idx, 'price'], oof[va_idx])\n",
    "    print(f\"  Fold SMAPE: {fold_smape:.4f}, Trees: {gbm.best_iteration}\")\n",
    "\n",
    "test_pred /= 5\n",
    "cv_smape = smape_np(train_df['price'], oof)\n",
    "print(f\"\\n5-Fold CV SMAPE: {cv_smape:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa1f29",
   "metadata": {},
   "source": [
    "# Holdout Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHoldout Validation...\")\n",
    "\n",
    "X_htr = np.hstack([emb_htr, hold_tr[feature_cols].values]).astype(np.float32)\n",
    "X_hva = np.hstack([emb_hva, hold_va[feature_cols].values]).astype(np.float32)\n",
    "\n",
    "train_set = lgb.Dataset(X_htr, np.log1p(hold_tr['price']))\n",
    "val_set = lgb.Dataset(X_hva, np.log1p(hold_va['price']))\n",
    "\n",
    "gbm_hold = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[val_set],\n",
    "    callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "pred_hold = np.expm1(gbm_hold.predict(X_hva))\n",
    "holdout_smape = smape_np(hold_va['price'], pred_hold)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"HOLDOUT SMAPE: {holdout_smape:.4f}\")\n",
    "print('='*60)\n",
    "\n",
    "if holdout_smape < 30:\n",
    "    print(\"âœ… TARGET ACHIEVED!\")\n",
    "elif holdout_smape < 35:\n",
    "    print(\"ðŸŽ¯ Very close to target!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdac56d",
   "metadata": {},
   "source": [
    "# Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip to training bounds\n",
    "lower = train_df['price'].quantile(0.001)\n",
    "upper = train_df['price'].quantile(0.999)\n",
    "final_pred = np.clip(test_pred, lower, upper)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(OUT_DIR/\"submission_minimal.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission saved!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CV SMAPE:      {cv_smape:.4f}\")\n",
    "print(f\"  Holdout SMAPE: {holdout_smape:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
