{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f77916c",
   "metadata": {},
   "source": [
    "# 01_mm_all_improved.ipynb â€“ Optimized Multimodal Price Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d289ee",
   "metadata": {},
   "source": [
    "# Cell 1: Setup (Environment, Device, Paths, Seeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d317044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0\n",
      "MPS built: True\n",
      "MPS available: True\n",
      "Device: mps\n",
      "Train CSV: /Users/adyasrivastava/Downloads/smart_pricing/dataset/train.csv\n",
      "Test  CSV: /Users/adyasrivastava/Downloads/smart_pricing/dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Local helpers\n",
    "from src.utils.seed import seed_everything\n",
    "from src.training.metrics import smape_np\n",
    "from src.data.load import read_csvs, make_folds\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = torch.device(os.getenv(\"DEVICE\", \"mps\"))\n",
    "TRAIN_CSV = str(project_root / os.getenv(\"TRAIN_CSV\", \"dataset/train.csv\"))\n",
    "TEST_CSV = str(project_root / os.getenv(\"TEST_CSV\", \"dataset/test.csv\"))\n",
    "IMG_DIR = project_root / Path(os.getenv(\"IMG_DIR\", \"data/processed/images\"))\n",
    "OUT_DIR = project_root / Path(os.getenv(\"OUT_DIR\", \"outputs\"))\n",
    "HF_HOME = project_root / Path(os.getenv(\"HF_HOME\", \".hf_cache\"))\n",
    "\n",
    "for p in [IMG_DIR, OUT_DIR, HF_HOME, OUT_DIR/\"oof\", OUT_DIR/\"test_preds\", OUT_DIR/\"reports\"]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Train CSV:\", TRAIN_CSV)\n",
    "print(\"Test  CSV:\", TEST_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271044a",
   "metadata": {},
   "source": [
    "# Cell 2: Load Data + Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f2eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: /Users/adyasrivastava/Downloads/smart_pricing/dataset/train.csv\n",
      "Loading test data from: /Users/adyasrivastava/Downloads/smart_pricing/dataset/test.csv\n",
      "\n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "Price statistics:\n",
      "  Min: $0.13\n",
      "  Max: $2796.00\n",
      "  Mean: $23.65\n",
      "  Median: $14.00\n",
      "\n",
      "Images available:\n",
      "  Train: 75000/75000 (100.0%)\n",
      "  Test: 75000/75000 (100.0%)\n",
      "\n",
      "Created 5 stratified folds:\n",
      "  Fold 0: 15000 samples, mean price: $23.46, median: $14.10\n",
      "  Fold 1: 15000 samples, mean price: $23.63, median: $13.99\n",
      "  Fold 2: 15000 samples, mean price: $23.47, median: $14.20\n",
      "  Fold 3: 15000 samples, mean price: $23.96, median: $13.99\n",
      "  Fold 4: 15000 samples, mean price: $23.73, median: $14.05\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "Extracting features from train set...\n",
      "Extracting features from test set...\n",
      "Extracting features from holdout sets...\n",
      "\n",
      "Extracted 20 features:\n",
      "['has_pack', 'has_multipack', 'has_oz', 'has_lb', 'has_kg', 'has_ml', 'has_l', 'num_count', 'max_number', 'min_number', 'sum_numbers', 'text_len', 'word_count', 'avg_word_len', 'num_digits', 'is_premium', 'is_budget', 'is_food', 'size_indicator', 'has_image']\n",
      "\n",
      "Feature shapes:\n",
      "  Train: (75000, 20)\n",
      "  Test: (75000, 20)\n",
      "============================================================\n",
      "\n",
      "Data shapes:\n",
      "  Train: 75000 | Test: 75000\n",
      "  Holdout train: 67500 | Holdout val: 7500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Read data\n",
    "train_df, test_df = read_csvs(TRAIN_CSV, TEST_CSV)\n",
    "train_df = make_folds(train_df, n_folds=5, seed=SEED)\n",
    "\n",
    "# Create holdout split\n",
    "y_log = np.log1p(train_df[\"price\"].clip(lower=0.0))\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED)\n",
    "holdout_tr_idx, holdout_va_idx = next(sss.split(np.zeros(len(y_log)), pd.qcut(y_log, q=20, duplicates=\"drop\")))\n",
    "hold_tr_df = train_df.iloc[holdout_tr_idx].copy()\n",
    "hold_va_df = train_df.iloc[holdout_va_idx].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract price-relevant features from catalog content\"\"\"\n",
    "    df = df.copy()\n",
    "    text = df['catalog_content'].fillna('')\n",
    "    \n",
    "    # Package indicators\n",
    "    df['has_pack'] = text.str.contains(r'\\bpack\\b|\\bcount\\b|\\bct\\b', case=False, regex=True).astype(int)\n",
    "    df['has_multipack'] = text.str.contains(r'\\d+\\s*pack', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Weight/volume indicators\n",
    "    df['has_oz'] = text.str.contains(r'\\boz\\b|ounce', case=False, regex=True).astype(int)\n",
    "    df['has_lb'] = text.str.contains(r'\\blb\\b|pound', case=False, regex=True).astype(int)\n",
    "    df['has_kg'] = text.str.contains(r'\\bkg\\b|kilogram', case=False, regex=True).astype(int)\n",
    "    df['has_ml'] = text.str.contains(r'\\bml\\b|milliliter', case=False, regex=True).astype(int)\n",
    "    df['has_l'] = text.str.contains(r'\\bl\\b|liter', case=False, regex=True).astype(int)\n",
    "    \n",
    "    # Extract numeric values\n",
    "    def extract_numbers(s):\n",
    "        nums = re.findall(r'\\d+\\.?\\d*', str(s))\n",
    "        return [float(n) for n in nums]\n",
    "    \n",
    "    df['num_count'] = text.apply(lambda x: len(extract_numbers(x)))\n",
    "    df['max_number'] = text.apply(lambda x: max(extract_numbers(x), default=0))\n",
    "    df['min_number'] = text.apply(lambda x: min(extract_numbers(x), default=0))\n",
    "    df['sum_numbers'] = text.apply(lambda x: sum(extract_numbers(x)))\n",
    "    \n",
    "    # Text statistics\n",
    "    df['text_len'] = text.str.len()\n",
    "    df['word_count'] = text.str.split().str.len()\n",
    "    df['avg_word_len'] = text.apply(lambda x: np.mean([len(w) for w in str(x).split()]) if x else 0)\n",
    "    df['num_digits'] = text.apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
    "    \n",
    "    # Brand/quality indicators\n",
    "    premium_brands = ['organic', 'premium', 'gourmet', 'artisan', 'natural', 'prime']\n",
    "    budget_brands = ['value', 'budget', 'economy', 'basic']\n",
    "    \n",
    "    df['is_premium'] = text.str.lower().str.contains('|'.join(premium_brands), regex=True).astype(int)\n",
    "    df['is_budget'] = text.str.lower().str.contains('|'.join(budget_brands), regex=True).astype(int)\n",
    "    \n",
    "    # Category indicators (food vs non-food)\n",
    "    food_keywords = ['food', 'snack', 'drink', 'beverage', 'grocery', 'eat']\n",
    "    df['is_food'] = text.str.lower().str.contains('|'.join(food_keywords), regex=True).astype(int)\n",
    "    \n",
    "    # Size indicators\n",
    "    size_keywords = {'small': 1, 'medium': 2, 'large': 3, 'xl': 4, 'jumbo': 5}\n",
    "    df['size_indicator'] = 0\n",
    "    for keyword, value in size_keywords.items():\n",
    "        df.loc[text.str.lower().str.contains(keyword, regex=False), 'size_indicator'] = value\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Extracting features from train set...\")\n",
    "train_df = extract_features(train_df)\n",
    "print(\"Extracting features from test set...\")\n",
    "test_df = extract_features(test_df)\n",
    "print(\"Extracting features from holdout sets...\")\n",
    "hold_tr_df = extract_features(hold_tr_df)\n",
    "hold_va_df = extract_features(hold_va_df)\n",
    "\n",
    "# Feature columns (only include features that actually exist)\n",
    "feature_cols = [\n",
    "    'has_pack','has_multipack','has_oz','has_lb','has_kg','has_ml','has_l',\n",
    "    'num_count','max_number','min_number','sum_numbers',\n",
    "    'text_len','word_count','avg_word_len','num_digits',\n",
    "    'is_premium','is_budget','is_food','size_indicator',\n",
    "    'has_image'  # This comes from read_csvs() function\n",
    "]\n",
    "\n",
    "print(f\"\\nExtracted {len(feature_cols)} features:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_df[feature_cols])\n",
    "test_feats = scaler.transform(test_df[feature_cols])\n",
    "holdtr_feats = scaler.transform(hold_tr_df[feature_cols])\n",
    "holdva_feats = scaler.transform(hold_va_df[feature_cols])\n",
    "\n",
    "print(f\"\\nFeature shapes:\")\n",
    "print(f\"  Train: {train_feats.shape}\")\n",
    "print(f\"  Test: {test_feats.shape}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Train: {len(train_df)} | Test: {len(test_df)}\")\n",
    "print(f\"  Holdout train: {len(hold_tr_df)} | Holdout val: {len(hold_va_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094fe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NEW CELL: Quantity & Unit Normalization ----\n",
    "import re\n",
    "\n",
    "_QTY_PAT = re.compile(r'(?P<num>\\d+(?:\\.\\d+)?)\\s*(?P<u>kg|g|gram|grams|lb|lbs|pound|pounds|oz|ounce|ounces|l|liter|litre|ml|milliliter|millilitre)\\b', re.I)\n",
    "_PACK_PAT = re.compile(r'(?:(?P<count>\\d+)\\s*[xÃ—*]\\s*)?(?P<size>\\d+(?:\\.\\d+)?)\\s*(?P<u>kg|g|gram|grams|lb|lbs|pound|pounds|oz|ounce|ounces|l|liter|litre|ml|milliliter|millilitre)\\b', re.I)\n",
    "_SINGLE_PACK = re.compile(r'\\b(\\d+)\\s*pack\\b|\\bpack\\s*of\\s*(\\d+)\\b|\\b(\\d+)\\s*ct\\b|\\b(\\d+)\\s*count\\b', re.I)\n",
    "\n",
    "def _to_grams(num, u):\n",
    "    u = u.lower()\n",
    "    if u in [\"kg\"]:   return num * 1000\n",
    "    if u in [\"g\",\"gram\",\"grams\"]: return num\n",
    "    if u in [\"lb\",\"lbs\",\"pound\",\"pounds\"]: return num * 453.592\n",
    "    if u in [\"oz\",\"ounce\",\"ounces\"]: return num * 28.3495\n",
    "    return None\n",
    "\n",
    "def _to_milliliters(num, u):\n",
    "    u = u.lower()\n",
    "    if u in [\"l\",\"liter\",\"litre\"]: return num * 1000\n",
    "    if u in [\"ml\",\"milliliter\",\"millilitre\"]: return num\n",
    "    # some products label oz fluid but ambiguous; skip to stay safe\n",
    "    return None\n",
    "\n",
    "def parse_qty_features(s):\n",
    "    s = str(s)\n",
    "    grams_list, ml_list = [], []\n",
    "    for m in _QTY_PAT.finditer(s):\n",
    "        val = float(m.group(\"num\"))\n",
    "        u   = m.group(\"u\")\n",
    "        g = _to_grams(val, u); ml = _to_milliliters(val, u)\n",
    "        if g is not None: grams_list.append(g)\n",
    "        if ml is not None: ml_list.append(ml)\n",
    "\n",
    "    # pack patterns like \"3 x 200 g\"\n",
    "    pack_count = 1\n",
    "    for m in _PACK_PAT.finditer(s):\n",
    "        count = m.group(\"count\")\n",
    "        if count: pack_count = max(pack_count, int(count))\n",
    "\n",
    "    # patterns like \"pack of 6\" or \"6 pack\"\n",
    "    for m in _SINGLE_PACK.finditer(s):\n",
    "        nums = [int(x) for x in m.groups() if x]\n",
    "        if nums: pack_count = max(pack_count, max(nums))\n",
    "\n",
    "    return {\n",
    "        \"content_mass_g\": np.max(grams_list) if grams_list else 0.0,\n",
    "        \"content_vol_ml\": np.max(ml_list) if ml_list else 0.0,\n",
    "        \"pack_count\": float(pack_count),\n",
    "        \"has_qty\": int(bool(grams_list or ml_list)),\n",
    "    }\n",
    "\n",
    "def add_qty_columns(df):\n",
    "    feats = df[\"catalog_content\"].fillna(\"\").apply(parse_qty_features).apply(pd.Series)\n",
    "    return pd.concat([df, feats], axis=1)\n",
    "\n",
    "# apply to all splits\n",
    "train_df = add_qty_columns(train_df)\n",
    "test_df  = add_qty_columns(test_df)\n",
    "hold_tr_df = add_qty_columns(hold_tr_df)\n",
    "hold_va_df = add_qty_columns(hold_va_df)\n",
    "\n",
    "# Heuristic: estimated number of units (mass or volume) to stabilize scale\n",
    "for d in (train_df, test_df, hold_tr_df, hold_va_df):\n",
    "    d[\"est_units\"] = np.where(d[\"content_mass_g\"]>0, d[\"content_mass_g\"]/100.0,\n",
    "                         np.where(d[\"content_vol_ml\"]>0, d[\"content_vol_ml\"]/100.0, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150f5ca",
   "metadata": {},
   "source": [
    "# Cell 3: Map Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7560c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image directory: /Users/adyasrivastava/Downloads/smart_pricing/data/processed/images\n",
      "Checking existing images...\n",
      "\n",
      "ðŸ“Š Images Available:\n",
      "   Train: 53,988/75,000 (72.0%)\n",
      "   Test:  3,787/75,000 (5.0%)\n",
      "\n",
      "âœ… Proceeding with available images\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "print(f\"Image directory: {IMG_DIR}\")\n",
    "\n",
    "def to_local_path(url: str) -> str:\n",
    "    p = urlparse(str(url))\n",
    "    name = os.path.basename(p.path) or \"na.jpg\"\n",
    "    return (IMG_DIR / name).as_posix()\n",
    "\n",
    "for df in (train_df, test_df, hold_tr_df, hold_va_df):\n",
    "    df[\"image_path\"] = df[\"image_link\"].fillna(\"\").map(to_local_path)\n",
    "\n",
    "# Check existing images\n",
    "print(\"Checking existing images...\")\n",
    "train_exists = sum([os.path.exists(p) for p in train_df[\"image_path\"].tolist()])\n",
    "test_exists = sum([os.path.exists(p) for p in test_df[\"image_path\"].tolist()])\n",
    "\n",
    "print(f\"\\nðŸ“Š Images Available:\")\n",
    "print(f\"   Train: {train_exists:,}/{len(train_df):,} ({100*train_exists/len(train_df):.1f}%)\")\n",
    "print(f\"   Test:  {test_exists:,}/{len(test_df):,} ({100*test_exists/len(test_df):.1f}%)\")\n",
    "print(f\"\\nâœ… Proceeding with available images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a35d0f",
   "metadata": {},
   "source": [
    "# Cell 4: MiniLM Text Embeddings + Feature Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b2083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with e5-small-v2 + Attention Pooling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e5-small+attn @ MPS: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [1:05:09<00:00,  6.67s/it] \n",
      "e5-small+attn @ MPS:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/586 [30:06<08:10,  3.86s/it]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4 (REPLACED): e5-small-v2 Text Embeddings with Attention Pooling\n",
    "# ============================================================\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import trange\n",
    "\n",
    "# --- Swap the encoder here ---\n",
    "MINILM = \"intfloat/e5-small-v2\"   # (Apache-2.0). Good for product-ish text.\n",
    "# If you want to try BGE instead:\n",
    "# MINILM = \"BAAI/bge-small-en-v1.5\"  # (MIT)  NOTE: set use_e5_prefix=False below for BGE.\n",
    "\n",
    "tok_minilm = AutoTokenizer.from_pretrained(MINILM, cache_dir=HF_HOME.as_posix())\n",
    "enc_minilm = AutoModel.from_pretrained(MINILM, cache_dir=HF_HOME.as_posix()).to(DEVICE)\n",
    "enc_minilm.eval()\n",
    "\n",
    "# --- Attention pooling (replaces mean pooling) ---\n",
    "class AttnPool(nn.Module):\n",
    "    def __init__(self, hidden: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # hidden_states: [B, T, H], attention_mask: [B, T] (1 for tokens, 0 for pad)\n",
    "        scores = self.proj(hidden_states).squeeze(-1)                 # [B, T]\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e9)        # mask pads\n",
    "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)          # [B, T, 1]\n",
    "        pooled = (hidden_states * weights).sum(dim=1)                 # [B, H]\n",
    "        return pooled\n",
    "\n",
    "attn_pool = AttnPool(enc_minilm.config.hidden_size).to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_minilm(texts, batch_size=128, max_len=256, cache_path=None, force_recompute=False, use_e5_prefix=True):\n",
    "    \"\"\"\n",
    "    Encode texts to L2-normalized sentence embeddings using:\n",
    "      - Frozen encoder (e5-small-v2 by default)\n",
    "      - Attention pooling over tokens\n",
    "      - MPS-friendly float32 output\n",
    "\n",
    "    Args:\n",
    "        use_e5_prefix: for e5 models, prepends \"query: \" to inputs (recommended).\n",
    "                       If you switch to BGE, set this False.\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path) if cache_path is not None else None\n",
    "    if cache_path and cache_path.exists() and not force_recompute:\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    out = []\n",
    "    desc = \"e5-small+attn @ MPS\" if use_e5_prefix else \"text+attn @ MPS\"\n",
    "    for i in trange(0, len(texts), batch_size, desc=desc):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        if use_e5_prefix:\n",
    "            # e5 models were trained with task prefixes; \"query: \" is standard\n",
    "            batch = [f\"query: {b}\" for b in batch]\n",
    "\n",
    "        enc = tok_minilm(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "        outputs = enc_minilm(**enc)               # last_hidden_state: [B, T, H]\n",
    "        last = outputs.last_hidden_state\n",
    "        pooled = attn_pool(last, enc[\"attention_mask\"])              # [B, H]\n",
    "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)   # L2 norm\n",
    "        out.append(pooled.to(torch.float32).cpu().numpy())\n",
    "\n",
    "    arr = np.vstack(out).astype(np.float32)\n",
    "    if cache_path:\n",
    "        np.save(cache_path, arr)\n",
    "    return arr\n",
    "\n",
    "# ---------------------------\n",
    "# Compute & cache embeddings\n",
    "# ---------------------------\n",
    "CACHE_TXT = Path(\"data/processed/improved_text_attn_e5\")\n",
    "CACHE_TXT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_texts  = train_df[\"catalog_content\"].fillna(\"\").astype(str).tolist()\n",
    "test_texts   = test_df[\"catalog_content\"].fillna(\"\").astype(str).tolist()\n",
    "holdtr_texts = hold_tr_df[\"catalog_content\"].fillna(\"\").astype(str).tolist()\n",
    "holdva_texts = hold_va_df[\"catalog_content\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "print(\"Encoding text with e5-small-v2 + Attention Pooling...\")\n",
    "Etxt_train_emb  = encode_minilm(train_texts,  cache_path=CACHE_TXT/\"e5_train_attn.npy\",  use_e5_prefix=True)\n",
    "Etxt_test_emb   = encode_minilm(test_texts,   cache_path=CACHE_TXT/\"e5_test_attn.npy\",   use_e5_prefix=True)\n",
    "Etxt_holdtr_emb = encode_minilm(holdtr_texts, cache_path=CACHE_TXT/\"e5_holdtr_attn.npy\", use_e5_prefix=True)\n",
    "Etxt_holdva_emb = encode_minilm(holdva_texts, cache_path=CACHE_TXT/\"e5_holdva_attn.npy\", use_e5_prefix=True)\n",
    "\n",
    "print(\"Shapes â€” text embeddings (attn pooled, e5):\")\n",
    "print(\"  train:\", Etxt_train_emb.shape, \" test:\", Etxt_test_emb.shape)\n",
    "print(\"  hold_tr:\", Etxt_holdtr_emb.shape, \" hold_va:\", Etxt_holdva_emb.shape)\n",
    "\n",
    "# IMPORTANT:\n",
    "# Do NOT concatenate handcrafted features here anymore.\n",
    "# You'll concatenate inside the fold loops (Cell 7 & 9) after fold-local scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1958d",
   "metadata": {},
   "source": [
    "# Cell 5: CLIP Embeddings (Text + Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eeb978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5 (REPLACED): CLIP Embeddings (Text + Image with TTA)\n",
    "# - Text: CLIP text features (no change)\n",
    "# - Image: TTA = average(original, horizontal flip)\n",
    "# ============================================================\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange  # trange used in text emb cell as well\n",
    "\n",
    "CLIP_NAME = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(CLIP_NAME, cache_dir=HF_HOME.as_posix())\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_NAME, cache_dir=HF_HOME.as_posix()).to(DEVICE)\n",
    "clip_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_text_emb(texts, batch_size=128, max_len=77, cache_path=None):\n",
    "    if cache_path and Path(cache_path).exists():\n",
    "        return np.load(cache_path)\n",
    "    feats = []\n",
    "    for i in trange(0, len(texts), batch_size, desc=\"CLIP text @ MPS\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = processor(\n",
    "            text=batch, images=None, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=max_len\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        tf = clip_model.get_text_features(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        tf = torch.nn.functional.normalize(tf, p=2, dim=1)\n",
    "        feats.append(tf.to(torch.float32).cpu().numpy())\n",
    "    arr = np.vstack(feats).astype(np.float32)\n",
    "    if cache_path:\n",
    "        np.save(cache_path, arr)\n",
    "    return arr\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_image_emb_tta(paths, batch_size=64, cache_path=None):\n",
    "    \"\"\"\n",
    "    Image TTA: embed original and horizontal-flipped images, then average.\n",
    "    Falls back to a zero vector if an image path cannot be opened.\n",
    "    \"\"\"\n",
    "    if cache_path and Path(cache_path).exists():\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    feats, batch_o, batch_f = [], [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal feats, batch_o, batch_f\n",
    "        if not batch_o:\n",
    "            return\n",
    "        # Processor returns dict; we only need pixel_values\n",
    "        inp1 = processor(images=batch_o, return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        inp2 = processor(images=batch_f, return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "\n",
    "        f1 = clip_model.get_image_features(pixel_values=inp1)\n",
    "        f2 = clip_model.get_image_features(pixel_values=inp2)\n",
    "\n",
    "        f1 = torch.nn.functional.normalize(f1, p=2, dim=1)\n",
    "        f2 = torch.nn.functional.normalize(f2, p=2, dim=1)\n",
    "\n",
    "        f = ((f1 + f2) / 2.0).to(torch.float32).cpu().numpy()\n",
    "        feats.append(f)\n",
    "\n",
    "        batch_o, batch_f = [], []\n",
    "\n",
    "    for p in tqdm(paths, desc=\"CLIP image TTA @ MPS\"):\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            img_flip = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            batch_o.append(img)\n",
    "            batch_f.append(img_flip)\n",
    "        except Exception:\n",
    "            # If image can't be read, append a single zero-vector with proper dim\n",
    "            # Probe the output dim from the modelâ€™s visual projection\n",
    "            z = np.zeros((1, clip_model.visual_projection.out_features), dtype=np.float32)\n",
    "            feats.append(z)\n",
    "            continue\n",
    "\n",
    "        if len(batch_o) >= batch_size:\n",
    "            flush()\n",
    "\n",
    "    flush()\n",
    "    arr = np.vstack(feats).astype(np.float32)\n",
    "    if cache_path:\n",
    "        np.save(cache_path, arr)\n",
    "    return arr\n",
    "\n",
    "# ------------- Caching paths -------------\n",
    "CACHE_CLIP = Path(\"data/processed/improved_clip\")\n",
    "CACHE_CLIP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Encoding text with CLIP...\")\n",
    "Ctxt_train  = clip_text_emb(train_texts,  cache_path=CACHE_CLIP/\"clip_txt_train.npy\")\n",
    "Ctxt_test   = clip_text_emb(test_texts,   cache_path=CACHE_CLIP/\"clip_txt_test.npy\")\n",
    "Ctxt_holdtr = clip_text_emb(holdtr_texts, cache_path=CACHE_CLIP/\"clip_txt_holdtr.npy\")\n",
    "Ctxt_holdva = clip_text_emb(holdva_texts, cache_path=CACHE_CLIP/\"clip_txt_holdva.npy\")\n",
    "\n",
    "print(\"Encoding images with CLIP (TTA: original + hflip)...\")\n",
    "Cimg_train  = clip_image_emb_tta(train_df[\"image_path\"].tolist(), cache_path=CACHE_CLIP/\"clip_img_train_tta.npy\")\n",
    "Cimg_test   = clip_image_emb_tta(test_df[\"image_path\"].tolist(),  cache_path=CACHE_CLIP/\"clip_img_test_tta.npy\")\n",
    "Cimg_holdtr = clip_image_emb_tta(hold_tr_df[\"image_path\"].tolist(), cache_path=CACHE_CLIP/\"clip_img_holdtr_tta.npy\")\n",
    "Cimg_holdva = clip_image_emb_tta(hold_va_df[\"image_path\"].tolist(), cache_path=CACHE_CLIP/\"clip_img_holdva_tta.npy\")\n",
    "\n",
    "print(f\"\\nCLIP embedding shapes:\")\n",
    "print(f\"  Text:  {Ctxt_train.shape}\")\n",
    "print(f\"  Image: {Cimg_train.shape} (TTA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075da74d",
   "metadata": {},
   "source": [
    "# Cell 6: Improved Model Architectures + Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model Architectures\n",
    "# =========================\n",
    "class ImprovedRegressor(nn.Module):\n",
    "    \"\"\"4-layer MLP for text/image heads with BatchNorm\"\"\"\n",
    "    def __init__(self, in_dim, hidden=512, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(in_dim)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden // 2)\n",
    "        self.fc3 = nn.Linear(hidden // 2, hidden // 4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden // 4)\n",
    "        self.fc4 = nn.Linear(hidden // 4, 1)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(self.bn1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(self.bn2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(self.bn3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "\n",
    "class FusionRegressor(nn.Module):\n",
    "    \"\"\"5-layer deep fusion network for multimodal inputs\"\"\"\n",
    "    def __init__(self, in_dim, hidden=768, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(in_dim)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden // 2)\n",
    "        self.fc3 = nn.Linear(hidden // 2, hidden // 4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden // 4)\n",
    "        self.fc4 = nn.Linear(hidden // 4, hidden // 8)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden // 8)\n",
    "        self.fc5 = nn.Linear(hidden // 8, 1)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(self.bn1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(self.bn2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(self.bn3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout(self.bn4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SMAPE-based early stopping\n",
    "# =========================\n",
    "class QuantileLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Pinball loss (quantile regression). q=0.5 is median, robust for heavy-tailed prices.\n",
    "    Use instead of Huber if your targets are spiky.\n",
    "    \"\"\"\n",
    "    def __init__(self, q=0.5):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "    def forward(self, pred, target):\n",
    "        # pred/target are in log-space here\n",
    "        e = target - pred\n",
    "        return torch.max(self.q * e, (self.q - 1) * e).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _val_smape_price(model, Xva, yva_log):\n",
    "    \"\"\"\n",
    "    Compute validation SMAPE in price space.\n",
    "    yva_log is log1p(price); convert both pred and target to price via expm1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    p_va_log = model(Xva)\n",
    "    p_va_price = torch.expm1(p_va_log).clamp_min(0).cpu().numpy()\n",
    "    y_va_price = torch.expm1(yva_log).clamp_min(0).cpu().numpy()\n",
    "    return smape_np(y_va_price, p_va_price)\n",
    "\n",
    "def train_head(\n",
    "    Xtr, ytr_log, Xva, yva_log, DEVICE,\n",
    "    epochs=25, bs=512, lr=1e-3, wd=1e-3,\n",
    "    hidden=512, drop=0.3, patience=7,\n",
    "    use_quantile=True, huber_delta=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Train MLP head (text) in log-price space; early-stop on validation SMAPE (price space).\n",
    "    \"\"\"\n",
    "    model = ImprovedRegressor(Xtr.shape[1], hidden=hidden, drop=drop).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "\n",
    "    loss_fn = QuantileLoss(q=0.5) if use_quantile else nn.HuberLoss(delta=huber_delta)\n",
    "\n",
    "    best_smape = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        idx = torch.randperm(Xtr.shape[0], device=DEVICE)\n",
    "        for i in range(0, len(idx), bs):\n",
    "            b = idx[i:i+bs]\n",
    "            pred = model(Xtr[b])\n",
    "            loss = loss_fn(pred, ytr_log[b])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        # --- Early stop on SMAPE (price space) ---\n",
    "        sm = _val_smape_price(model, Xva, yva_log)\n",
    "        if sm < best_smape:\n",
    "            best_smape = sm\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_va = model(Xva)  # still log-space; caller will expm1 when needed\n",
    "    return model, p_va\n",
    "\n",
    "def train_fusion(\n",
    "    Xtr, ytr_log, Xva, yva_log, DEVICE,\n",
    "    epochs=30, bs=512, lr=5e-4, wd=1e-3,\n",
    "    hidden=768, drop=0.3, patience=8,\n",
    "    use_quantile=True, huber_delta=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Train fusion MLP (text+image+features) in log-price; early-stop on SMAPE.\n",
    "    \"\"\"\n",
    "    model = FusionRegressor(Xtr.shape[1], hidden=hidden, drop=drop).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "\n",
    "    loss_fn = QuantileLoss(q=0.5) if use_quantile else nn.HuberLoss(delta=huber_delta)\n",
    "\n",
    "    best_smape = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        idx = torch.randperm(Xtr.shape[0], device=DEVICE)\n",
    "        for i in range(0, len(idx), bs):\n",
    "            b = idx[i:i+bs]\n",
    "            pred = model(Xtr[b])\n",
    "            loss = loss_fn(pred, ytr_log[b])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        # --- Early stop on SMAPE (price space) ---\n",
    "        sm = _val_smape_price(model, Xva, yva_log)\n",
    "        if sm < best_smape:\n",
    "            best_smape = sm\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_va = model(Xva)  # log-space\n",
    "    return model, p_va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fa365",
   "metadata": {},
   "source": [
    "# Cell 7: Train Text Head (5-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Text Head (MiniLM + Features)  [fold-local scaling]\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "oof_txt = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_txt_accum = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"\\nFold {fold+1}/5...\")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    # --- Fold-local scaler: fit ONLY on TRAIN rows of this fold ---\n",
    "    scaler_f = StandardScaler()\n",
    "    feats_tr = scaler_f.fit_transform(train_df.loc[tr.index, feature_cols])\n",
    "    feats_va = scaler_f.transform(train_df.loc[va.index, feature_cols])\n",
    "    feats_te = scaler_f.transform(test_df[feature_cols])\n",
    "\n",
    "    # We concatenate embeddings (from MiniLM) with the per-fold scaled features\n",
    "    # NOTE: use *raw* text embeddings here (Etxt_*_emb), not the globally-concatenated Etxt_train\n",
    "    Xtr_np = np.hstack([Etxt_train_emb[tr.index], feats_tr]).astype(np.float32)\n",
    "    Xva_np = np.hstack([Etxt_train_emb[va.index], feats_va]).astype(np.float32)\n",
    "    Xte_np = np.hstack([Etxt_test_emb,            feats_te]).astype(np.float32)\n",
    "\n",
    "    # To device\n",
    "    Xtr = torch.from_numpy(Xtr_np).to(DEVICE)\n",
    "    Xva = torch.from_numpy(Xva_np).to(DEVICE)\n",
    "    Xte = torch.from_numpy(Xte_np).to(DEVICE)\n",
    "\n",
    "    ytr = torch.from_numpy(np.log1p(tr.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "    yva = torch.from_numpy(np.log1p(va.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "    # Train (uses your train_head; consider SMAPE-based ES from earlier suggestions)\n",
    "    model, p_va = train_head(\n",
    "        Xtr, ytr, Xva, yva, DEVICE,\n",
    "        epochs=25, bs=512, lr=1e-3, wd=1e-3,\n",
    "        hidden=512, drop=0.3\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p_te = model(Xte)\n",
    "\n",
    "    oof_txt[va.index] = torch.expm1(p_va).cpu().numpy()\n",
    "    test_txt_accum += torch.expm1(p_te).cpu().numpy()\n",
    "\n",
    "sm_txt = smape_np(train_df.price.values, np.clip(oof_txt, 0.0, None))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Text Head OOF SMAPE: {sm_txt:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "np.save(OUT_DIR/\"oof\"/\"txt_improved.npy\", oof_txt)\n",
    "pd.DataFrame({\n",
    "    \"sample_id\": test_df.sample_id,\n",
    "    \"price\": np.clip(test_txt_accum/5.0, 0.0, None)\n",
    "}).to_csv(OUT_DIR/\"test_preds\"/\"txt_improved.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Holdout evaluation (no leak)\n",
    "# ----------------------------\n",
    "print(\"\\nHoldout evaluation (fold-local style for holdout)...\")\n",
    "\n",
    "# Fit a brand-new scaler ONLY on holdout-train\n",
    "scaler_h = StandardScaler()\n",
    "fe_htr = scaler_h.fit_transform(hold_tr_df[feature_cols])\n",
    "fe_hva = scaler_h.transform(hold_va_df[feature_cols])\n",
    "\n",
    "Xtr_h = torch.from_numpy(np.hstack([Etxt_holdtr_emb, fe_htr]).astype(np.float32)).to(DEVICE)\n",
    "Xva_h = torch.from_numpy(np.hstack([Etxt_holdva_emb, fe_hva]).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "ytr_h = torch.from_numpy(np.log1p(hold_tr_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "yva_h = torch.from_numpy(np.log1p(hold_va_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "model_txt_hold, p_va_h = train_head(\n",
    "    Xtr_h, ytr_h, Xva_h, yva_h, DEVICE,\n",
    "    epochs=25, bs=512, lr=1e-3, wd=1e-3\n",
    ")\n",
    "pred_hold_txt = torch.expm1(p_va_h).cpu().numpy()\n",
    "sm_txt_hold = smape_np(hold_va_df.price.values, np.clip(pred_hold_txt, 0.0, None))\n",
    "print(f\"Text Head Holdout SMAPE: {sm_txt_hold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365384d5",
   "metadata": {},
   "source": [
    "# Cell 8: Train Image Head (5-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Image Head (CLIP Image)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "oof_img = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_img_accum = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"\\nFold {fold+1}/5...\")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    # Convert to float32 explicitly before creating tensors (MPS doesn't support float64)\n",
    "    Xtr = torch.from_numpy(Cimg_train[tr.index].astype(np.float32)).to(DEVICE)\n",
    "    Xva = torch.from_numpy(Cimg_train[va.index].astype(np.float32)).to(DEVICE)\n",
    "    Xte = torch.from_numpy(Cimg_test.astype(np.float32)).to(DEVICE)\n",
    "\n",
    "    ytr = torch.from_numpy(np.log1p(tr.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "    yva = torch.from_numpy(np.log1p(va.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "    model, p_va = train_head(Xtr, ytr, Xva, yva, DEVICE, epochs=25, bs=512, lr=8e-4,\n",
    "                             wd=1e-3, hidden=512, drop=0.3)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        p_te = model(Xte)\n",
    "\n",
    "    oof_img[va.index] = torch.expm1(p_va).cpu().numpy()\n",
    "    test_img_accum += torch.expm1(p_te).cpu().numpy()\n",
    "\n",
    "sm_img = smape_np(train_df.price.values, np.clip(oof_img, 0.0, None))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Image Head OOF SMAPE: {sm_img:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "np.save(OUT_DIR/\"oof\"/\"img_improved.npy\", oof_img)\n",
    "pd.DataFrame({\n",
    "    \"sample_id\": test_df.sample_id,\n",
    "    \"price\": np.clip(test_img_accum/5.0, 0.0, None)\n",
    "}).to_csv(OUT_DIR/\"test_preds\"/\"img_improved.csv\", index=False)\n",
    "\n",
    "# Holdout evaluation\n",
    "print(\"\\nHoldout evaluation...\")\n",
    "Xtr = torch.from_numpy(Cimg_holdtr.astype(np.float32)).to(DEVICE)\n",
    "Xva = torch.from_numpy(Cimg_holdva.astype(np.float32)).to(DEVICE)\n",
    "ytr = torch.from_numpy(np.log1p(hold_tr_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "yva = torch.from_numpy(np.log1p(hold_va_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "model_img_hold, p_va = train_head(Xtr, ytr, Xva, yva, DEVICE, epochs=25, bs=512, lr=8e-4, wd=1e-3)\n",
    "pred_hold_img = torch.expm1(p_va).cpu().numpy()\n",
    "sm_img_hold = smape_np(hold_va_df.price.values, np.clip(pred_hold_img, 0.0, None))\n",
    "print(f\"Image Head Holdout SMAPE: {sm_img_hold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce999da1",
   "metadata": {},
   "source": [
    "# Cell 9: Train Multimodal Fusion (5-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Multimodal Fusion (CLIP Text + Image + Features)  [fold-local scaling]\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "oof_mm = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_mm_accum = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "# We will build Xtr/Xva/Xte inside each fold with a fold-local scaler\n",
    "# so we don't precompute mm_train/mm_test with globally-scaled features.\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"\\nFold {fold+1}/5...\")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    # --- Fold-local scaler on handcrafted features ---\n",
    "    scaler_f = StandardScaler()\n",
    "    feats_tr = scaler_f.fit_transform(train_df.loc[tr.index, feature_cols])\n",
    "    feats_va = scaler_f.transform(train_df.loc[va.index, feature_cols])\n",
    "    feats_te = scaler_f.transform(test_df[feature_cols])\n",
    "\n",
    "    # Concatenate CLIP text + CLIP image + per-fold scaled features\n",
    "    Xtr_np = np.hstack([Ctxt_train[tr.index], Cimg_train[tr.index], feats_tr]).astype(np.float32)\n",
    "    Xva_np = np.hstack([Ctxt_train[va.index], Cimg_train[va.index], feats_va]).astype(np.float32)\n",
    "    Xte_np = np.hstack([Ctxt_test,            Cimg_test,            feats_te]).astype(np.float32)\n",
    "\n",
    "    Xtr = torch.from_numpy(Xtr_np).to(DEVICE)\n",
    "    Xva = torch.from_numpy(Xva_np).to(DEVICE)\n",
    "    Xte = torch.from_numpy(Xte_np).to(DEVICE)\n",
    "\n",
    "    ytr = torch.from_numpy(np.log1p(tr.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "    yva = torch.from_numpy(np.log1p(va.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "    model, p_va = train_fusion(\n",
    "        Xtr, ytr, Xva, yva, DEVICE,\n",
    "        epochs=30, bs=512, lr=5e-4, wd=1e-3,\n",
    "        hidden=768, drop=0.3\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p_te = model(Xte)\n",
    "\n",
    "    oof_mm[va.index] = torch.expm1(p_va).cpu().numpy()\n",
    "    test_mm_accum += torch.expm1(p_te).cpu().numpy()\n",
    "\n",
    "sm_mm = smape_np(train_df.price.values, np.clip(oof_mm, 0.0, None))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Multimodal Fusion OOF SMAPE: {sm_mm:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "np.save(OUT_DIR/\"oof\"/\"mm_improved.npy\", oof_mm)\n",
    "pd.DataFrame({\n",
    "    \"sample_id\": test_df.sample_id,\n",
    "    \"price\": np.clip(test_mm_accum/5.0, 0.0, None)\n",
    "}).to_csv(OUT_DIR/\"test_preds\"/\"mm_improved.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Holdout evaluation (no leak)\n",
    "# ----------------------------\n",
    "print(\"\\nHoldout evaluation (fold-local style for holdout)...\")\n",
    "\n",
    "scaler_h = StandardScaler()\n",
    "fe_htr = scaler_h.fit_transform(hold_tr_df[feature_cols])\n",
    "fe_hva = scaler_h.transform(hold_va_df[feature_cols])\n",
    "\n",
    "Xtr_h = torch.from_numpy(np.hstack([Ctxt_holdtr, Cimg_holdtr, fe_htr]).astype(np.float32)).to(DEVICE)\n",
    "Xva_h = torch.from_numpy(np.hstack([Ctxt_holdva, Cimg_holdva, fe_hva]).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "ytr_h = torch.from_numpy(np.log1p(hold_tr_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "yva_h = torch.from_numpy(np.log1p(hold_va_df.price.values.clip(min=0.0)).astype(np.float32)).to(DEVICE)\n",
    "\n",
    "model_mm_hold, p_va_h = train_fusion(\n",
    "    Xtr_h, ytr_h, Xva_h, yva_h, DEVICE,\n",
    "    epochs=30, bs=512, lr=5e-4, wd=1e-3\n",
    ")\n",
    "pred_hold_mm = torch.expm1(p_va_h).cpu().numpy()\n",
    "sm_mm_hold = smape_np(hold_va_df.price.values, np.clip(pred_hold_mm, 0.0, None))\n",
    "print(f\"Multimodal Fusion Holdout SMAPE: {sm_mm_hold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326b862",
   "metadata": {},
   "source": [
    "# Cell 10: Train Gradient Boosting Models (XGBoost + LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10 (REPLACED): Gradient Boosting Models (XGB, LGB, LGB-Tweedie)\n",
    "# No scaling (trees don't need it) + add Tweedie on raw price\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Gradient Boosting Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install if needed\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    print(\"Installing XGBoost and LightGBM...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\", \"lightgbm\", \"-q\"])\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build GBM feature matrices: [MiniLM txt emb | CLIP txt | CLIP img | raw handcrafted]\n",
    "# We AVOID global scaling here to prevent CV leakage (trees don't need scaling).\n",
    "# ------------------------------------------------------------------\n",
    "dense_train = train_df[feature_cols].astype(np.float32).values\n",
    "dense_test  = test_df[feature_cols].astype(np.float32).values\n",
    "dense_htr   = hold_tr_df[feature_cols].astype(np.float32).values\n",
    "dense_hva   = hold_va_df[feature_cols].astype(np.float32).values\n",
    "\n",
    "X_full_train  = np.hstack([Etxt_train_emb,  Ctxt_train,  Cimg_train,  dense_train]).astype(np.float32)\n",
    "X_full_test   = np.hstack([Etxt_test_emb,   Ctxt_test,   Cimg_test,   dense_test]).astype(np.float32)\n",
    "X_full_holdtr = np.hstack([Etxt_holdtr_emb, Ctxt_holdtr, Cimg_holdtr, dense_htr]).astype(np.float32)\n",
    "X_full_holdva = np.hstack([Etxt_holdva_emb, Ctxt_holdva, Cimg_holdva, dense_hva]).astype(np.float32)\n",
    "\n",
    "print(f\"Full GBM feature dimension: {X_full_train.shape[1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5-fold XGBoost (log-price) ---------------------------------\n",
    "# ============================================================\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "oof_xgb = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_xgb_accum = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"  Fold {fold+1}/5...\", end=\" \")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED + fold,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_full_train[tr.index],\n",
    "        np.log1p(tr.price.values),\n",
    "        eval_set=[(X_full_train[va.index], np.log1p(va.price.values))],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    oof_xgb[va.index]   = np.expm1(model.predict(X_full_train[va.index]))\n",
    "    test_xgb_accum     += np.expm1(model.predict(X_full_test))\n",
    "    print(\"Done\")\n",
    "\n",
    "sm_xgb = smape_np(train_df.price.values, np.clip(oof_xgb, 0.0, None))\n",
    "print(f\"\\nXGBoost OOF SMAPE: {sm_xgb:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5-fold LightGBM (log-price) --------------------------------\n",
    "# ============================================================\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "oof_lgb = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_lgb_accum = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"  Fold {fold+1}/5...\", end=\" \")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=-1,           # let leaves control depth\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=20,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED + fold,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_full_train[tr.index],\n",
    "        np.log1p(tr.price.values),\n",
    "        eval_set=[(X_full_train[va.index], np.log1p(va.price.values))],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "\n",
    "    oof_lgb[va.index]   = np.expm1(model.predict(X_full_train[va.index]))\n",
    "    test_lgb_accum     += np.expm1(model.predict(X_full_test))\n",
    "    print(f\"Done (trees: {model.best_iteration_})\")\n",
    "\n",
    "sm_lgb = smape_np(train_df.price.values, np.clip(oof_lgb, 0.0, None))\n",
    "print(f\"\\nLightGBM OOF SMAPE: {sm_lgb:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5-fold LightGBM Tweedie (RAW price) ------------------------\n",
    "# ============================================================\n",
    "print(\"\\nTraining LightGBM Tweedie...\")\n",
    "oof_lgb_twd = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_lgb_twd_acc = np.zeros(len(test_df), dtype=np.float32)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"  Fold {fold+1}/5...\", end=\" \")\n",
    "    tr = train_df[train_df.fold != fold]\n",
    "    va = train_df[train_df.fold == fold]\n",
    "\n",
    "    lgb_tweedie = lgb.LGBMRegressor(\n",
    "        objective=\"tweedie\",\n",
    "        tweedie_variance_power=1.2,   # try 1.1â€“1.6 if you tune\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED + fold,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train on RAW price\n",
    "    lgb_tweedie.fit(\n",
    "        X_full_train[tr.index], train_df.loc[tr.index, \"price\"].values,\n",
    "        eval_set=[(X_full_train[va.index], train_df.loc[va.index, \"price\"].values)],\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    oof_lgb_twd[va.index]   = np.clip(lgb_tweedie.predict(X_full_train[va.index]), 0.0, None)\n",
    "    test_lgb_twd_acc       += np.clip(lgb_tweedie.predict(X_full_test),            0.0, None)\n",
    "    print(\"Done\")\n",
    "\n",
    "sm_lgb_twd = smape_np(train_df.price.values, np.clip(oof_lgb_twd, 0.0, None))\n",
    "print(f\"\\nLightGBM Tweedie OOF SMAPE: {sm_lgb_twd:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save GBM OOFs + Test predictions\n",
    "# ------------------------------------------------------------\n",
    "np.save(OUT_DIR/\"oof\"/\"xgb.npy\",         oof_xgb)\n",
    "np.save(OUT_DIR/\"oof\"/\"lgb.npy\",         oof_lgb)\n",
    "np.save(OUT_DIR/\"oof\"/\"lgb_tweedie.npy\", oof_lgb_twd)\n",
    "\n",
    "pd.DataFrame({\"sample_id\": test_df.sample_id, \"price\": np.clip(test_xgb_accum/5.0,      0.0, None)}).to_csv(OUT_DIR/\"test_preds\"/\"xgb.csv\",          index=False)\n",
    "pd.DataFrame({\"sample_id\": test_df.sample_id, \"price\": np.clip(test_lgb_accum/5.0,      0.0, None)}).to_csv(OUT_DIR/\"test_preds\"/\"lgb.csv\",          index=False)\n",
    "pd.DataFrame({\"sample_id\": test_df.sample_id, \"price\": np.clip(test_lgb_twd_acc/5.0,    0.0, None)}).to_csv(OUT_DIR/\"test_preds\"/\"lgb_tweedie.csv\",  index=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Holdout evaluation for all three\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nHoldout evaluation for GBMs...\")\n",
    "\n",
    "# XGBoost (log-price)\n",
    "xgb_hold = xgb.XGBRegressor(\n",
    "    n_estimators=2000, learning_rate=0.03, max_depth=8,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=SEED,\n",
    "    tree_method='hist', n_jobs=-1\n",
    ")\n",
    "xgb_hold.fit(\n",
    "    X_full_holdtr, np.log1p(hold_tr_df.price.values),\n",
    "    eval_set=[(X_full_holdva, np.log1p(hold_va_df.price.values))],\n",
    "    verbose=False\n",
    ")\n",
    "pred_hold_xgb = np.expm1(xgb_hold.predict(X_full_holdva))\n",
    "sm_xgb_hold = smape_np(hold_va_df.price.values, np.clip(pred_hold_xgb, 0.0, None))\n",
    "\n",
    "# LightGBM (log-price)\n",
    "lgb_hold = lgb.LGBMRegressor(\n",
    "    n_estimators=2000, learning_rate=0.03, num_leaves=63,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=SEED, verbose=-1\n",
    ")\n",
    "lgb_hold.fit(\n",
    "    X_full_holdtr, np.log1p(hold_tr_df.price.values),\n",
    "    eval_set=[(X_full_holdva, np.log1p(hold_va_df.price.values))],\n",
    "    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    ")\n",
    "pred_hold_lgb = np.expm1(lgb_hold.predict(X_full_holdva))\n",
    "sm_lgb_hold = smape_np(hold_va_df.price.values, np.clip(pred_hold_lgb, 0.0, None))\n",
    "\n",
    "# LightGBM Tweedie (RAW price)\n",
    "lgb_twd_hold = lgb.LGBMRegressor(\n",
    "    objective=\"tweedie\",\n",
    "    tweedie_variance_power=1.2,\n",
    "    n_estimators=4000, learning_rate=0.02, num_leaves=63,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=SEED, verbose=-1\n",
    ")\n",
    "lgb_twd_hold.fit(\n",
    "    X_full_holdtr, hold_tr_df.price.values,\n",
    "    eval_set=[(X_full_holdva, hold_va_df.price.values)],\n",
    "    callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    ")\n",
    "pred_hold_lgb_twd = np.clip(lgb_twd_hold.predict(X_full_holdva), 0.0, None)\n",
    "sm_lgb_twd_hold = smape_np(hold_va_df.price.values, pred_hold_lgb_twd)\n",
    "\n",
    "print(f\"XGBoost Holdout SMAPE:        {sm_xgb_hold:.4f}\")\n",
    "print(f\"LightGBM (log) Holdout SMAPE: {sm_lgb_hold:.4f}\")\n",
    "print(f\"LightGBM Tweedie Holdout:     {sm_lgb_twd_hold:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5485e",
   "metadata": {},
   "source": [
    "# Cell 11: Ensemble Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4022198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11 (REPLACED): Ensemble Optimization + Local Refinement\n",
    "# - Coarse grid search (step=0.05)\n",
    "# - Local refinement around the best (Â±0.10, step=0.01)\n",
    "# - Auto-include lgb_tweedie and stack_ridge if available\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Collect OOF predictions\n",
    "# ------------------------------------------------------------\n",
    "oofs = {\n",
    "    \"text\": oof_txt,\n",
    "    \"img\":  oof_img,\n",
    "    \"mm\":   oof_mm,\n",
    "    \"xgb\":  oof_xgb,\n",
    "    \"lgb\":  oof_lgb,\n",
    "}\n",
    "\n",
    "# Optionally include LightGBM Tweedie if trained in Cell 10\n",
    "if \"oof_lgb_twd\" in globals():\n",
    "    oofs[\"lgb_tweedie\"] = oof_lgb_twd\n",
    "\n",
    "# Optionally include Ridge stacker if you added it\n",
    "# (variables typically named oof_stack and/or saved test CSV in Cell \"stacker\")\n",
    "if \"oof_stack\" in globals():\n",
    "    oofs[\"stack_ridge\"] = oof_stack\n",
    "\n",
    "# Ground-truth\n",
    "y = train_df[\"price\"].values\n",
    "\n",
    "def grid_weights(keys, step=0.05):\n",
    "    \"\"\"Generate all weight combinations that sum to 1 with given step.\"\"\"\n",
    "    n = len(keys)\n",
    "    w = np.arange(0, 1 + 1e-9, step)\n",
    "\n",
    "    if n == 1:\n",
    "        yield {keys[0]: 1.0}\n",
    "        return\n",
    "\n",
    "    def rec(prefix, remain, total):\n",
    "        if remain == 1:\n",
    "            yield prefix + [total]\n",
    "            return\n",
    "        for a in w:\n",
    "            if a > total:\n",
    "                break\n",
    "            yield from rec(prefix + [a], remain - 1, total - a)\n",
    "\n",
    "    for weights in rec([], n, 1.0):\n",
    "        yield dict(zip(keys, weights))\n",
    "\n",
    "keys = list(oofs.keys())\n",
    "X = np.stack([oofs[k] for k in keys], axis=1)\n",
    "\n",
    "print(f\"\\nSearching {len(keys)}-model ensemble space (coarse step=0.05)...\")\n",
    "best = (1e9, None)\n",
    "tested = 0\n",
    "\n",
    "for wmap in grid_weights(keys, step=0.05):\n",
    "    w = np.array([wmap[k] for k in keys], dtype=np.float32)\n",
    "    pred = (X * w[None, :]).sum(axis=1)\n",
    "    sm = smape_np(y, np.clip(pred, 0.0, None))\n",
    "    if sm < best[0]:\n",
    "        best = (sm, wmap)\n",
    "    tested += 1\n",
    "    if tested % 10000 == 0:\n",
    "        print(f\"  Tested {tested} combos, best so far: {best[0]:.4f}\")\n",
    "\n",
    "best_oof_smape, best_w = best\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Best (coarse) Ensemble OOF SMAPE: {best_oof_smape:.4f}\")\n",
    "print(\"Coarse optimal weights:\")\n",
    "for k, v in best_w.items():\n",
    "    print(f\"  {k:12s}: {v:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Local refinement around the best (Â±0.10, step 0.01)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nRefining around the coarse optimum (Â±0.10, step=0.01)...\")\n",
    "base = np.array([best_w[k] for k in keys], dtype=np.float32)\n",
    "\n",
    "def local_refine(base, radius=0.10, step=0.01):\n",
    "    grid = np.arange(-radius, radius + 1e-9, step)\n",
    "    best = (1e9, base.copy())\n",
    "    # To keep the loops tractable, adjust first min(3, len(base)) weights locally.\n",
    "    k = min(3, len(base))\n",
    "    for d0 in grid:\n",
    "        for d1 in grid:\n",
    "            # if k==2, skip d2 loop; if k>=3, include it\n",
    "            if k >= 3:\n",
    "                ds = (d0, d1)\n",
    "                for d2 in grid:\n",
    "                    d = base.copy()\n",
    "                    d[:3] += np.array([d0, d1, d2])\n",
    "                    d = np.clip(d, 0, None)\n",
    "                    s = d.sum()\n",
    "                    if s <= 0: \n",
    "                        continue\n",
    "                    d /= s\n",
    "                    sm = smape_np(y, np.clip((X * d[None, :]).sum(axis=1), 0.0, None))\n",
    "                    if sm < best[0]:\n",
    "                        best = (sm, d.copy())\n",
    "            else:\n",
    "                d = base.copy()\n",
    "                d[:2] += np.array([d0, d1])\n",
    "                d = np.clip(d, 0, None)\n",
    "                s = d.sum()\n",
    "                if s <= 0: \n",
    "                    continue\n",
    "                d /= s\n",
    "                sm = smape_np(y, np.clip((X * d[None, :]).sum(axis=1), 0.0, None))\n",
    "                if sm < best[0]:\n",
    "                    best = (sm, d.copy())\n",
    "    return best\n",
    "\n",
    "ref_sm, ref_w = local_refine(base, radius=0.10, step=0.01)\n",
    "best_oof_smape = float(ref_sm)\n",
    "best_w = dict(zip(keys, ref_w.tolist()))\n",
    "\n",
    "print(f\"Refined OOF SMAPE: {best_oof_smape:.4f}\")\n",
    "print(\"Refined weights:\")\n",
    "for k in keys:\n",
    "    print(f\"  {k:12s}: {best_w[k]:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Holdout ensemble using the refined weights\n",
    "# ------------------------------------------------------------\n",
    "hold_pred_map = {}\n",
    "\n",
    "# Mandatory holdout preds (from Cells 7â€“10)\n",
    "if \"pred_hold_txt\" in globals(): hold_pred_map[\"text\"] = pred_hold_txt\n",
    "if \"pred_hold_img\" in globals(): hold_pred_map[\"img\"]  = pred_hold_img\n",
    "if \"pred_hold_mm\" in globals():  hold_pred_map[\"mm\"]   = pred_hold_mm\n",
    "if \"pred_hold_xgb\" in globals(): hold_pred_map[\"xgb\"]  = pred_hold_xgb\n",
    "if \"pred_hold_lgb\" in globals(): hold_pred_map[\"lgb\"]  = pred_hold_lgb\n",
    "\n",
    "# Optional additions (if present)\n",
    "if \"pred_hold_lgb_twd\" in globals():\n",
    "    hold_pred_map[\"lgb_tweedie\"] = pred_hold_lgb_twd\n",
    "if \"pred_hold_stack\" in globals():\n",
    "    hold_pred_map[\"stack_ridge\"] = pred_hold_stack\n",
    "\n",
    "# Ensure we only ensemble keys for which we have BOTH OOF and holdout\n",
    "keys = [k for k in keys if k in hold_pred_map]\n",
    "w = np.array([best_w[k] for k in keys], dtype=np.float32)\n",
    "hold_stack = np.stack([hold_pred_map[k] for k in keys], axis=1)\n",
    "hold_pred = (hold_stack * w[None, :]).sum(axis=1)\n",
    "sm_hold_ens = smape_np(hold_va_df.price.values, np.clip(hold_pred, 0.0, None))\n",
    "\n",
    "print(f\"\\nEnsemble Holdout SMAPE (refined): {sm_hold_ens:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save ensemble report (keys + weights + scores)\n",
    "# ------------------------------------------------------------\n",
    "report = {\n",
    "    \"oof_smape\": float(best_oof_smape),\n",
    "    \"holdout_smape\": float(sm_hold_ens),\n",
    "    \"keys\": keys,  # Order matters for Cell 12 test blending\n",
    "    \"weights\": [float(best_w[k]) for k in keys],\n",
    "    \"weights_map\": {k: float(best_w[k]) for k in keys},\n",
    "    \"individual_oof_scores\": {\n",
    "        \"text\": float(sm_txt) if \"sm_txt\" in globals() else None,\n",
    "        \"img\":  float(sm_img) if \"sm_img\" in globals() else None,\n",
    "        \"mm\":   float(sm_mm)  if \"sm_mm\"  in globals() else None,\n",
    "        \"xgb\":  float(sm_xgb) if \"sm_xgb\" in globals() else None,\n",
    "        \"lgb\":  float(sm_lgb) if \"sm_lgb\" in globals() else None,\n",
    "        \"lgb_tweedie\": float(sm_lgb_twd) if \"sm_lgb_twd\" in globals() else None,\n",
    "        \"stack_ridge\": float(sm_stack)   if \"sm_stack\"   in globals() else None,\n",
    "    }\n",
    "}\n",
    "(OUT_DIR/\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_DIR/\"reports\"/\"ensemble_improved.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Saved refined ensemble report to\", (OUT_DIR/\"reports\"/\"ensemble_improved.json\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f57d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "stack_names = [\"text\",\"img\",\"mm\",\"xgb\",\"lgb\"]\n",
    "# add if present:\n",
    "if (OUT_DIR/\"oof\"/\"lgb_tweedie.npy\").exists():\n",
    "    stack_names.append(\"lgb_tweedie\")\n",
    "\n",
    "X_stack = np.stack([np.log1p(np.clip(oofs[n], 0, 1e6)) for n in stack_names], axis=1)\n",
    "y_stack = np.log1p(np.clip(train_df[\"price\"].values, 0, 1e6))\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=SEED)\n",
    "ridge.fit(X_stack, y_stack)\n",
    "\n",
    "# Build test stack in same order\n",
    "def load_test_csv(name):\n",
    "    # Map model names to actual CSV filenames\n",
    "    file_map = {\n",
    "        \"text\": \"txt_improved\",\n",
    "        \"img\": \"img_improved\",\n",
    "        \"mm\": \"mm_improved\",\n",
    "        \"xgb\": \"xgb\",\n",
    "        \"lgb\": \"lgb\",\n",
    "        \"lgb_tweedie\": \"lgb_tweedie\"\n",
    "    }\n",
    "    filename = file_map.get(name, name)\n",
    "    preds = pd.read_csv(OUT_DIR/\"test_preds\"/f\"{filename}.csv\")[\"price\"].values\n",
    "    return np.clip(preds, 0, 1e6)  # Clip to avoid inf in log1p\n",
    "T_stack = np.stack([np.log1p(load_test_csv(n)) for n in stack_names], axis=1)\n",
    "\n",
    "oof_stack = np.expm1(ridge.predict(X_stack))\n",
    "sm_stack = smape_np(train_df[\"price\"].values, np.clip(oof_stack, 0.0, None))\n",
    "print(\"Ridge stacker OOF SMAPE:\", sm_stack)\n",
    "\n",
    "test_stack = np.expm1(ridge.predict(T_stack))\n",
    "pd.DataFrame({\"sample_id\": test_df.sample_id, \"price\": np.clip(test_stack, 0.0, None)}).to_csv(OUT_DIR/\"test_preds\"/\"stack_ridge.csv\", index=False)\n",
    "\n",
    "# add to ensemble pool\n",
    "oofs[\"stack_ridge\"] = oof_stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b0c98",
   "metadata": {},
   "source": [
    "# Cell 12: Generate Final Predictions + Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Final Ensemble Predictions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load individual test predictions\n",
    "t_text = pd.read_csv(OUT_DIR/\"test_preds\"/\"txt_improved.csv\")\n",
    "t_img = pd.read_csv(OUT_DIR/\"test_preds\"/\"img_improved.csv\")\n",
    "t_mm = pd.read_csv(OUT_DIR/\"test_preds\"/\"mm_improved.csv\")\n",
    "t_xgb = pd.read_csv(OUT_DIR/\"test_preds\"/\"xgb.csv\")\n",
    "t_lgb = pd.read_csv(OUT_DIR/\"test_preds\"/\"lgb.csv\")\n",
    "\n",
    "# Merge all predictions\n",
    "sub = t_text.rename(columns={\"price\": \"price_text\"}) \\\n",
    "    .merge(t_img.rename(columns={\"price\": \"price_img\"}), on=\"sample_id\") \\\n",
    "    .merge(t_mm.rename(columns={\"price\": \"price_mm\"}), on=\"sample_id\") \\\n",
    "    .merge(t_xgb.rename(columns={\"price\": \"price_xgb\"}), on=\"sample_id\") \\\n",
    "    .merge(t_lgb.rename(columns={\"price\": \"price_lgb\"}), on=\"sample_id\")\n",
    "\n",
    "# Apply ensemble weights\n",
    "weights = np.array([best_w[k] for k in [\"text\", \"img\", \"mm\", \"xgb\", \"lgb\"]], dtype=np.float32)\n",
    "sub[\"price\"] = sub[[\"price_text\", \"price_img\", \"price_mm\", \"price_xgb\", \"price_lgb\"]].values.dot(weights)\n",
    "\n",
    "# Post-processing: clip to training distribution bounds\n",
    "print(\"\\nApplying post-processing...\")\n",
    "lower_bound = train_df['price'].quantile(0.001)\n",
    "upper_bound = train_df['price'].quantile(0.999)\n",
    "print(f\"  Clipping to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "sub[\"price\"] = sub[\"price\"].clip(lower_bound, upper_bound)\n",
    "\n",
    "# Final submission\n",
    "final = sub[[\"sample_id\", \"price\"]].copy()\n",
    "final.to_csv(OUT_DIR/\"test_preds\"/\"ensemble_improved.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ… Final submission saved to: {(OUT_DIR/'test_preds'/'ensemble_improved.csv').as_posix()}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(final.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ IMPROVED PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Final Results Summary:\")\n",
    "print(f\"\\nIndividual Model OOF SMAPE:\")\n",
    "print(f\"  Text Head:          {sm_txt:.4f}\")\n",
    "print(f\"  Image Head:         {sm_img:.4f}\")\n",
    "print(f\"  Multimodal Fusion:  {sm_mm:.4f}\")\n",
    "print(f\"  XGBoost:            {sm_xgb:.4f}\")\n",
    "print(f\"  LightGBM:           {sm_lgb:.4f}\")\n",
    "print(f\"\\nðŸ† Ensemble OOF SMAPE:     {best_oof_smape:.4f}\")\n",
    "print(f\"ðŸŽ¯ Ensemble Holdout SMAPE: {sm_hold_ens:.4f}\")\n",
    "print(f\"\\nEnsemble Weights:\")\n",
    "for k, v in best_w.items():\n",
    "    print(f\"  {k:15s}: {v:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if target achieved\n",
    "if sm_hold_ens < 40:\n",
    "    print(f\"\\nðŸŽ‰ TARGET ACHIEVED! Holdout SMAPE = {sm_hold_ens:.4f} < 40\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Target not met. Holdout SMAPE = {sm_hold_ens:.4f}\")\n",
    "    print(\"\\nSuggestions for further improvement:\")\n",
    "    print(\"  1. Use larger CLIP model (ViT-L/14 instead of ViT-B/32)\")\n",
    "    print(\"  2. Add TF-IDF features from text\")\n",
    "    print(\"  3. Implement pseudo-labeling on test set\")\n",
    "    print(\"  4. Add more domain-specific features (brand recognition, category)\")\n",
    "    print(\"  5. Use CatBoost in addition to XGBoost/LightGBM\")\n",
    "    print(\"  6. Increase training epochs with better early stopping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
